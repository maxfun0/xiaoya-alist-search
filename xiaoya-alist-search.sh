#!/bin/bash

# åˆå§‹ç”¨æˆ·è¾“å…¥
echo "è¯·è¾“å…¥å°é›…alistçš„dockeråç§° (é»˜è®¤: xiaoya):"
read -r DOCKER_NAME
DOCKER_NAME=${DOCKER_NAME:-xiaoya}

echo "è¯·è¾“å…¥å°é›…alistçš„åœ°å€ (æ ¼å¼: http://192.168.0.254:5678):"
read -r AList_URL

echo "è¯·è¾“å…¥æ–°å»ºdockerçš„å®‰è£…è·¯å¾„:"
read -r INSTALL_PATH

# æ£€æŸ¥å¹¶åˆ›å»ºç›®å½•
if [ ! -d "$INSTALL_PATH" ]; then
    mkdir -p "$INSTALL_PATH"
    echo "ç›®å½• $INSTALL_PATH å·²åˆ›å»ºã€‚"
else
    echo "ç›®å½• $INSTALL_PATH å·²å­˜åœ¨ã€‚"
fi

while true; do
    echo "è¯·è¾“å…¥æ–°å»ºdockerçš„åç§°:"
    read -r NEW_DOCKER_NAME

    # æ£€æŸ¥æ˜¯å¦æœ‰åŒåçš„docker
    if [ "$(docker ps -a --filter "name=^/${NEW_DOCKER_NAME}$" --format '{{.Names}}')" ]; then
        echo "Error: Dockeråç§°å·²å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚"
    else
        break
    fi
done

echo "è¯·è¾“å…¥éœ€è¦ä½¿ç”¨çš„ç«¯å£:"
read -r PORT

# è®¡ç®—æ–°ç«¯å£
NEW_PORT=$((PORT + 1))

echo "é€‰æ‹©ä½¿ç”¨webdavæˆ–è€…alistå¥—å¨ƒå°é›…alist:"
echo "å¦‚æœä½ æƒ³åˆ†äº«å¤šäººä½¿ç”¨ï¼Œæœ‰å…¬ç½‘ipï¼Œä¸Šè¡Œå®½å¸¦è‡³å°‘æœ‰30-50å…†ï¼Œå¯ä»¥ä½¿ç”¨webdavã€‚"
echo "å¦‚æœä½ ä½¿ç”¨è®¾å¤‡ä¸å¤šï¼Œæ²¡æœ‰å…¬ç½‘ipï¼Œé€‰æ‹©alistã€‚"
echo "è¾“å…¥1é€‰æ‹©webdavï¼Œè¾“å…¥2é€‰æ‹©alist (é»˜è®¤: 1):"
read -r SERVICE_TYPE
SERVICE_TYPE=${SERVICE_TYPE:-1}

# æ ¹æ®é€‰æ‹©çš„æœåŠ¡ç±»å‹è¿›è¡Œä¸åŒçš„é…ç½®
if [ "$SERVICE_TYPE" -eq 1 ]; then
    echo "å°é›…alistæ˜¯å¦å¼€å¯å¼ºåˆ¶ç™»å…¥? (1: æœªå¼€å¯, 2: å¼€å¯, é»˜è®¤: 1):"
    read -r FORCE_LOGIN
    FORCE_LOGIN=${FORCE_LOGIN:-1}

    if [ "$FORCE_LOGIN" -eq 2 ]; then
        echo "è¯·è¾“å…¥å¯†ç :"
        read -r PASSWORD
    else
        PASSWORD="guest"
    fi
else
    # å¯¹äºé€‰æ‹© AList çš„æƒ…å†µï¼Œå¯†ç è®¾ç½®å›ºå®š
    FORCE_LOGIN=1
    PASSWORD=""
fi

# åˆ›å»ºdocker-composeæ–‡ä»¶å¹¶å¯åŠ¨å®¹å™¨
COMPOSE_FILE=$(mktemp)
cat <<EOF > "$COMPOSE_FILE"
version: '3.3'
services:
    alist:
        image: 'xhofe/alist:latest'
        container_name: $NEW_DOCKER_NAME
        volumes:
            - '$INSTALL_PATH:/opt/alist/data'
        ports:
            - '$PORT:5244'
            - '$NEW_PORT:5245'
        environment:
            - PUID=0
            - PGID=0
            - UMASK=022
        restart: unless-stopped
EOF

docker compose -f "$COMPOSE_FILE" up -d

# ç­‰å¾…å®¹å™¨å¯åŠ¨
sleep 15

# è·å–tokenä¿¡æ¯ï¼ˆä»…åœ¨é€‰æ‹© AList æ—¶éœ€è¦ï¼‰
if [ "$SERVICE_TYPE" -eq 2 ]; then
    TOKEN=$(docker exec -i "$DOCKER_NAME" sqlite3 data/data.db <<EOF
select value from x_setting_items where key = 'token';
EOF
)
    echo "è·å–åˆ°çš„token: $TOKEN"
fi

# åœæ­¢å®¹å™¨
docker stop "$NEW_DOCKER_NAME"

# ç­‰å¾…å®¹å™¨åœæ­¢
sleep 5

if [ ! -f "$INSTALL_PATH/data.db" ]; then
    docker start "$NEW_DOCKER_NAME"
    sleep 60
    docker stop "$NEW_DOCKER_NAME"
fi

# å¤‡ä»½æ•°æ®åº“
mkdir -p "$INSTALL_PATH/alist-temp"
cp "$INSTALL_PATH/data.db" "$INSTALL_PATH/alist-temp/data.db.bak"

# æå–txtæ–‡ä»¶
docker cp "${DOCKER_NAME}:/index/." "$INSTALL_PATH/alist-temp/"

# æ•°æ®åº“æ“ä½œ
PYTHON_SCRIPT=$(cat <<EOF
import sqlite3
import json
import datetime
import logging
import os

# é…ç½®æ—¥å¿—è¾“å‡ºæ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

DB_FILE = "$INSTALL_PATH/data.db"  # ç¡®ä¿åœ¨ Bash è„šæœ¬ä¸­æ›¿æ¢ä¸ºå®é™…è·¯å¾„
TXT_FOLDER = "$INSTALL_PATH/alist-temp"  # æŒ‡å®šå¤„ç†çš„ TXT æ–‡ä»¶å¤¹
SEQ_INSERT = "INSERT OR REPLACE INTO sqlite_sequence (name, seq) VALUES ('x_storages', 19);"
BATCH_SIZE = 1000  # æ‰¹é‡æ’å…¥å¤§å°

def insert_data(service_type, token, address, password, force_login):
    logging.info("Connecting to database...")
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    # æ‰§è¡Œåºåˆ—æ’å…¥
    logging.info("Inserting sequence...")
    cursor.execute(SEQ_INSERT)

    # å®šä¹‰æ•°æ®æ’å…¥å‡½æ•°
    if service_type == 1:
        if force_login == 1:  # æœªå¼€å¯å¼ºåˆ¶ç™»é™†
            username = "guest"
            password_input = "guest_Api789"
        else:
            username = "dav"
            password_input = password

        data = [
            (1, '/ä½“è‚²', 1, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/ä½“è‚²","tls_insecure_skip_verify":False}),
            (2, '/åŠ¨æ¼«', 2, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/åŠ¨æ¼«","tls_insecure_skip_verify":False}),
            (3, '/æ•™è‚²', 3, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/æ•™è‚²","tls_insecure_skip_verify":False}),
            (4, '/æ•´ç†ä¸­', 4, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/æ•´ç†ä¸­","tls_insecure_skip_verify":False}),
            (5, '/æ›²è‰º', 5, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/æ›²è‰º","tls_insecure_skip_verify":False}),
            (6, '/æœ‰å£°ä¹¦', 6, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/æœ‰å£°ä¹¦","tls_insecure_skip_verify":False}),
            (7, '/æ¯æ—¥æ›´æ–°', 7, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/æ¯æ—¥æ›´æ–°","tls_insecure_skip_verify":False}),
            (8, '/æ¸¸æˆ', 8, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/æ¸¸æˆ","tls_insecure_skip_verify":False}),
            (9, '/ç”µå­ä¹¦', 9, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/ç”µå­ä¹¦","tls_insecure_skip_verify":False}),
            (10, '/ç”µå½±', 10, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/ç”µå½±","tls_insecure_skip_verify":False}),
            (11, '/ç”µè§†å‰§', 11, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/ç”µè§†å‰§","tls_insecure_skip_verify":False}),
            (12, '/çºªå½•ç‰‡', 12, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/çºªå½•ç‰‡","tls_insecure_skip_verify":False}),
            (13, '/ç»¼è‰º', 13, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/ç»¼è‰º","tls_insecure_skip_verify":False}),
            (14, '/èµ„æ–™', 14, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/èµ„æ–™","tls_insecure_skip_verify":False}),
            (15, '/éŸ³ä¹', 15, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/éŸ³ä¹","tls_insecure_skip_verify":False}),
            (16, '/ğŸŒ€æˆ‘çš„å¤¸å…‹åˆ†äº«', 16, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/ğŸŒ€æˆ‘çš„å¤¸å…‹åˆ†äº«","tls_insecure_skip_verify":False}),
            (17, '/ğŸ·ï¸æˆ‘çš„115åˆ†äº«', 17, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/ğŸ·ï¸æˆ‘çš„115åˆ†äº«","tls_insecure_skip_verify":False}),
            (18, '/ğŸ“ºç”»è´¨æ¼”ç¤ºæµ‹è¯•ï¼ˆ4Kï¼Œ8Kï¼ŒHDRï¼ŒDolbyï¼‰', 18, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/ğŸ“ºç”»è´¨æ¼”ç¤ºæµ‹è¯•ï¼ˆ4Kï¼Œ8Kï¼ŒHDRï¼ŒDolbyï¼‰","tls_insecure_skip_verify":False}),
            (19, '/ğŸ•¸ï¸æˆ‘çš„PikPakåˆ†äº«', 19, 'WebDav', 30, 'work', {"vendor":"other","address":address,"username":username,"password":password_input,"root_folder_path":"/dav/ğŸ•¸ï¸æˆ‘çš„PikPakåˆ†äº«","tls_insecure_skip_verify":False})
        ]
    elif service_type == 2:
        data = [
            (1, '/ä½“è‚²', 1, 'AList V3', 30, 'work', {"root_folder_path":"/ä½“è‚²","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (2, '/åŠ¨æ¼«', 2, 'AList V3', 30, 'work', {"root_folder_path":"/åŠ¨æ¼«","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (3, '/æ•™è‚²', 3, 'AList V3', 30, 'work', {"root_folder_path":"/æ•™è‚²","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (4, '/æ•´ç†ä¸­', 4, 'AList V3', 30, 'work', {"root_folder_path":"/æ•´ç†ä¸­","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":False}),
            (5, '/æ›²è‰º', 5, 'AList V3', 30, 'work', {"root_folder_path":"/æ›²è‰º","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (6, '/æœ‰å£°ä¹¦', 6, 'AList V3', 30, 'work', {"root_folder_path":"/æœ‰å£°ä¹¦","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (7, '/æ¯æ—¥æ›´æ–°', 7, 'AList V3', 30, 'work', {"root_folder_path":"/æ¯æ—¥æ›´æ–°","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (8, '/æ¸¸æˆ', 8, 'AList V3', 30, 'work', {"root_folder_path":"/æ¸¸æˆ","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (9, '/ç”µå­ä¹¦', 9, 'AList V3', 30, 'work', {"root_folder_path":"/ç”µå­ä¹¦","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (10, '/ç”µå½±', 10, 'AList V3', 30, 'work', {"root_folder_path":"/ç”µå½±","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (11, '/ç”µè§†å‰§', 11, 'AList V3', 30, 'work', {"root_folder_path":"/ç”µè§†å‰§","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (12, '/çºªå½•ç‰‡', 12, 'AList V3', 30, 'work', {"root_folder_path":"/çºªå½•ç‰‡","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (13, '/ç»¼è‰º', 13, 'AList V3', 30, 'work', {"root_folder_path":"/ç»¼è‰º","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (14, '/èµ„æ–™', 14, 'AList V3', 30, 'work', {"root_folder_path":"/èµ„æ–™","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (15, '/éŸ³ä¹', 15, 'AList V3', 30, 'work', {"root_folder_path":"/éŸ³ä¹","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (16, '/ğŸŒ€æˆ‘çš„å¤¸å…‹åˆ†äº«', 16, 'AList V3', 30, 'work', {"root_folder_path":"/ğŸŒ€æˆ‘çš„å¤¸å…‹åˆ†äº«","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (17, '/ğŸ·ï¸æˆ‘çš„115åˆ†äº«', 17, 'AList V3', 30, 'work', {"root_folder_path":"/ğŸ·ï¸æˆ‘çš„115åˆ†äº«","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (18, '/ğŸ“ºç”»è´¨æ¼”ç¤ºæµ‹è¯•ï¼ˆ4Kï¼Œ8Kï¼ŒHDRï¼ŒDolbyï¼‰', 18, 'AList V3', 30, 'work', {"root_folder_path":"/ğŸ“ºç”»è´¨æ¼”ç¤ºæµ‹è¯•ï¼ˆ4Kï¼Œ8Kï¼ŒHDRï¼ŒDolbyï¼‰","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True}),
            (19, '/ğŸ•¸ï¸æˆ‘çš„PikPakåˆ†äº«', 19, 'AList V3', 30, 'work', {"root_folder_path":"/ğŸ•¸ï¸æˆ‘çš„PikPakåˆ†äº«","url":address,"meta_password":"","username":"","password":"","token":token,"pass_ua_to_upsteam":True})
        ]

    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # æ’å…¥æ¯æ¡æ•°æ®è®°å½•
    for entry in data:
        id, mount_path, order, driver, cache_expiration, status, addition = entry
        addition_json = json.dumps(addition).replace('"', '""')
        webdav_policy = 'native_proxy' if driver == 'WebDav' else '302_redirect'

        logging.info(f"æ’å…¥æŒ‚è½½è·¯å¾„çš„æ•°æ®: {mount_path}")

        # æ‰§è¡Œæ’å…¥è¯­å¥
        cursor.execute(f"""
            INSERT INTO x_storages (id, mount_path, [order], driver, cache_expiration, status,
            addition, remark, modified, disabled, enable_sign, order_by, order_direction,
            extract_folder, web_proxy, webdav_policy, proxy_range, down_proxy_url)
            VALUES ({id}, '{mount_path}', {order}, '{driver}', {cache_expiration}, '{status}', "{addition_json}", '', 
            '{current_time}', 0, 0, 'name', 'asc', 'front', 0, '{webdav_policy}', 0, '')
        """)

    # æäº¤æ›´æ”¹å¹¶å…³é—­è¿æ¥
    conn.commit()
    logging.info("æ•°æ®æ’å…¥æˆåŠŸä¸”æ›´æ”¹å·²æäº¤.")
    conn.close()

def clear_table():
    """æ¸…ç©º x_search_nodes è¡¨"""
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        cursor.execute('DELETE FROM x_search_nodes')
        conn.commit()
        logging.info("x_search_nodes è¡¨å·²æ¸…ç©ºã€‚")

def create_table():
    """åˆ›å»º x_search_nodes è¡¨"""
    logging.info("åˆ›å»ºæ•°æ®åº“è¡¨ x_search_nodes.")
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS x_search_nodes (
            parent TEXT,
            name TEXT,
            is_dir NUMERIC,
            size INTEGER
        )
        ''')
        conn.commit()

def check_and_create_index():
    """æ£€æŸ¥ç´¢å¼•å¹¶åˆ›å»ºç´¢å¼•"""
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_x_search_nodes_parent ON x_search_nodes (parent, name)")
        conn.commit()
        logging.info("æ£€æŸ¥å¹¶ç¡®ä¿ç´¢å¼• idx_x_search_nodes_parent å­˜åœ¨ã€‚")

def insert_data_batch(data_batch):
    """æ‰¹é‡æ’å…¥æ•°æ®åˆ° x_search_nodes è¡¨"""
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        cursor.executemany('''
        INSERT INTO x_search_nodes (parent, name, is_dir, size)
        VALUES (?, ?, ?, ?)
        ''', data_batch)
        conn.commit()

def process_file(file_path):
    """å¤„ç†å•ä¸ª TXT æ–‡ä»¶å¹¶æå–æ•°æ®"""
    data_batch = []
    logging.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            for line in file:
                line = line.strip()
                if '#' in line:
                    parts = line.split('#')
                    path_with_file = parts[0].replace('./', '')
                else:
                    path_with_file = line.replace('./', '')

                last_slash_index = path_with_file.rfind('/')

                if last_slash_index != -1:
                    parent = '/' + path_with_file[:last_slash_index]
                    name = path_with_file[last_slash_index + 1:]

                    data_batch.append((parent, name, 1, 0))

                    # å½“è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶ï¼Œæ’å…¥æ•°æ®å¹¶æ¸…ç©ºæ‰¹æ¬¡åˆ—è¡¨
                    if len(data_batch) >= BATCH_SIZE:
                        insert_data_batch(data_batch)
                        data_batch.clear()

        # æ’å…¥å‰©ä½™çš„æ•°æ®
        if data_batch:
            insert_data_batch(data_batch)
    except Exception as e:
        logging.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

def process_all_files_in_folder(folder_path):
    """å¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ TXT æ–‡ä»¶"""
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            file_path = os.path.join(folder_path, filename)
            process_file(file_path)

def deduplicate_records():
    """å»é‡ x_search_nodes è¡¨ä¸­çš„è®°å½•"""
    logging.info("å¼€å§‹å»é‡ x_search_nodes è¡¨ä¸­çš„è®°å½•.")
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        cursor.execute('''
            DELETE FROM x_search_nodes 
            WHERE rowid NOT IN (
                SELECT MIN(rowid)
                FROM x_search_nodes
                GROUP BY parent, name
            )
        ''')
        conn.commit()
        logging.info("å»é‡å®Œæˆã€‚")
       
def update_search_index():
    """æ›´æ–° x_setting_items è¡¨ä¸­çš„ search_index"""
    logging.info("æ›´æ–° x_setting_items è¡¨ä¸­çš„ search_index.")
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        cursor.execute('''
            UPDATE x_setting_items
            SET value = 'database_non_full_text'
            WHERE key = 'search_index'
        ''')
        conn.commit()
        logging.info("search_index çš„å€¼å·²æ›´æ–°ä¸º database_non_full_textã€‚")
        
if __name__ == '__main__':
    # æ›´æ–° search_index
    update_search_index()  
    
    # è¿™é‡Œæ˜¯ç”¨äºè°ƒç”¨æ•°æ®æ’å…¥çš„éƒ¨åˆ†
    insert_data($SERVICE_TYPE, "$TOKEN", "$AList_URL", "$PASSWORD", $FORCE_LOGIN)

    create_table()     # åˆ›å»ºè¡¨
    clear_table()      # æ¸…ç©ºè¡¨
    process_all_files_in_folder(TXT_FOLDER)  # å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ TXT æ–‡ä»¶
    deduplicate_records()  # å»é‡
    check_and_create_index()  # æ£€æŸ¥å¹¶åˆ›å»ºç´¢å¼•
    logging.info("æ•°æ®å¤„ç†å®Œæˆã€‚")
EOF
)

echo "$PYTHON_SCRIPT" > "$INSTALL_PATH/alist-temp/db_script.py"

python3 "$INSTALL_PATH/alist-temp/db_script.py"

echo "æ˜¯å¦åˆ›å»ºè‡ªåŠ¨æ›´æ–°çš„è„šæœ¬? (Y/N):"
read -r CREATE_SCRIPT

if [ "$CREATE_SCRIPT" == "Y" ] || [ "$CREATE_SCRIPT" == "y" ]; then
    # åˆ›å»ºæ›´æ–°è„šæœ¬
    UPDATE_SCRIPT="$INSTALL_PATH/update_script.sh"
    cat <<EOF > "$UPDATE_SCRIPT"
#!/bin/bash
docker stop "$NEW_DOCKER_NAME"
docker cp "${DOCKER_NAME}:/index/." "$INSTALL_PATH/alist-temp/"

# ç­‰å¾…å®¹å™¨åœæ­¢
sleep 3

python3 <<PYTHON_SCRIPT
import sqlite3
import logging
import os

# é…ç½®æ—¥å¿—è¾“å‡ºæ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

DB_FILE = "$INSTALL_PATH/data.db"  # æ•°æ®åº“æ–‡ä»¶è·¯å¾„
TXT_FOLDER = "$INSTALL_PATH/alist-temp"  # TXT æ–‡ä»¶å¤¹è·¯å¾„
BATCH_SIZE = 1000  # æ‰¹é‡æ’å…¥å¤§å°

def clear_table():
    """æ¸…ç©º x_search_nodes è¡¨"""
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        cursor.execute('DELETE FROM x_search_nodes')
        conn.commit()
        logging.info("x_search_nodes è¡¨å·²æ¸…ç©ºã€‚")

def insert_data_batch(data_batch):
    """æ‰¹é‡æ’å…¥æ•°æ®åˆ° x_search_nodes è¡¨"""
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        cursor.executemany('''
        INSERT INTO x_search_nodes (parent, name, is_dir, size)
        VALUES (?, ?, ?, ?)
        ''', data_batch)
        conn.commit()

def process_file(file_path):
    """å¤„ç†å•ä¸ª TXT æ–‡ä»¶å¹¶æå–æ•°æ®"""
    data_batch = []
    logging.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            for line in file:
                line = line.strip()
                if '#' in line:
                    parts = line.split('#')
                    path_with_file = parts[0].replace('./', '')
                else:
                    path_with_file = line.replace('./', '')

                last_slash_index = path_with_file.rfind('/')

                if last_slash_index != -1:
                    parent = '/' + path_with_file[:last_slash_index]
                    name = path_with_file[last_slash_index + 1:]

                    data_batch.append((parent, name, 1, 0))

                    # å½“è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶ï¼Œæ’å…¥æ•°æ®å¹¶æ¸…ç©ºæ‰¹æ¬¡åˆ—è¡¨
                    if len(data_batch) >= BATCH_SIZE:
                        insert_data_batch(data_batch)
                        data_batch.clear()

        # æ’å…¥å‰©ä½™çš„æ•°æ®
        if data_batch:
            insert_data_batch(data_batch)
    except Exception as e:
        logging.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

def process_all_files_in_folder(folder_path):
    """å¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ TXT æ–‡ä»¶"""
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            file_path = os.path.join(folder_path, filename)
            process_file(file_path)

def deduplicate_records():
    """å»é‡ x_search_nodes è¡¨ä¸­çš„è®°å½•"""
    logging.info("å¼€å§‹å»é‡ x_search_nodes è¡¨ä¸­çš„è®°å½•.")
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        cursor.execute('''
            DELETE FROM x_search_nodes 
            WHERE rowid NOT IN (
                SELECT MIN(rowid)
                FROM x_search_nodes
                GROUP BY parent, name
            )
        ''')
        conn.commit()
        logging.info("å»é‡å®Œæˆã€‚")

if __name__ == '__main__':
    clear_table()  # æ¸…ç©ºè¡¨
    process_all_files_in_folder(TXT_FOLDER)  # å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ TXT æ–‡ä»¶
    deduplicate_records()  # å»é‡
PYTHON_SCRIPT

docker start "$NEW_DOCKER_NAME"
EOF

    chmod +x "$UPDATE_SCRIPT"
    echo "æ›´æ–°è„šæœ¬å·²åˆ›å»º: $UPDATE_SCRIPT"
    echo -e "\033[31mè¯·å°†è¯¥è„šæœ¬æ·»åŠ åˆ°ä»»åŠ¡è®¡åˆ’ä¸­ä»¥è®¾ç½®æ›´æ–°é¢‘ç‡ã€‚\033[0m"
fi

# å¯åŠ¨å®¹å™¨ä»¥ç¡®ä¿å®ƒåœ¨è¿è¡Œ
docker start "$NEW_DOCKER_NAME"
if [ $? -ne 0 ]; then
    echo "Error: Failed to start Docker container $NEW_DOCKER_NAME."
    exit 1
fi

# åˆå§‹åŒ–å®Œæˆä¿¡æ¯
docker exec -it "$NEW_DOCKER_NAME" ./alist admin random
if [ $? -ne 0 ]; then
    echo "Error: Failed to execute command in Docker container $NEW_DOCKER_NAME."
    exit 1
fi

echo -e "æ‰§è¡Œå®Œæ¯•,ä»¥ä¸Šä¸ºåˆå§‹è´¦å·å¯†ç ï¼Œç™»å½•å\033[31må°½å¿«ä¿®æ”¹å¯†ç \033[0m"
